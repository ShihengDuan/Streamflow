{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Sequential, Model, save_model, load_model\n",
    "from scipy.stats import pearsonr\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "from keras.backend import slice\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pickle\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(station): ## Station in String\n",
    "    flow = np.load('../../usgsflow_'+station+'.npy')\n",
    "    precip = np.load('../../NLDAS_precip_'+station+'.npy')\n",
    "    srad = np.load('../../NLDAS_srad_'+station+'.npy')\n",
    "    tmax = np.load('../../NLDAS_tmax_'+station+'.npy')\n",
    "    y = np.array(flow).reshape(-1, 1)\n",
    "    indx = np.where(y>=0)[0]\n",
    "    # print(precip.shape)\n",
    "    date = np.load('../../usgsdate_'+station+'.npy', allow_pickle=True)\n",
    "    x = np.concatenate((precip, srad, tmax), axis=1)\n",
    "    return x, y\n",
    "def nse(y_pred, y_true):\n",
    "    nse = 1-np.sum((y_pred-y_true)**2)/np.sum((y_true-np.mean(y_true))**2)\n",
    "    return nse\n",
    "def dataset_ld(x,y,W,L):\n",
    "    obs = x.shape[0]\n",
    "    features = x.shape[1]\n",
    "    a = np.zeros([obs-W-L+1, W, features])\n",
    "    b = np.zeros([obs-W-L+1, 1])\n",
    "    for i in range(obs-W-L+1):\n",
    "        a[i,:,:] = x[i:i+W,:]\n",
    "        b[i,:] = y[i+W+L-1,0]    \n",
    "    return a, b\n",
    "def train_test_pre(x, y):\n",
    "    xtrain = x[:10000]; xtest = x[10000:]\n",
    "    ytrain = y[:10000]; ytest = y[10000:]\n",
    "    xscale = StandardScaler().fit(xtrain)\n",
    "    yscale = StandardScaler().fit(ytrain)\n",
    "    Xtrain = xscale.transform(xtrain); Xtest = xscale.transform(xtest)\n",
    "    Ytrain = yscale.transform(ytrain); Ytest = yscale.transform(ytest)\n",
    "    return Xtrain, Xtest, Ytrain, Ytest, xscale, yscale\n",
    "def custom_loss(y_true, y_pred):\n",
    "    s1 = K.sum((y_pred-y_true)**2)/K.sum((y_true-K.mean(y_true))**2)\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(W,L):\n",
    "    x_in = keras.layers.Input(shape=(W,3)) # Batch, Length, Dimension\n",
    "    ## Block 1\n",
    "    x_tp = keras.layers.Conv1D(kernel_size=7, filters=40, dilation_rate=1, padding='causal')(x_in)\n",
    "    # x_tp = keras.layers.BatchNormalization()(x_tp)\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.6)(x_tp)\n",
    "    x_tp = keras.layers.Conv1D(kernel_size=7, filters=40, dilation_rate=1, padding='causal')(x_tp)\n",
    "    # x_tp = keras.layers.BatchNormalization()(x_tp)\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.6)(x_tp)\n",
    "    ## add res for block 1\n",
    "    x_res = keras.layers.Conv1D(kernel_size=1, filters=40, dilation_rate=1, padding='causal')(x_in)\n",
    "    x_tp = keras.layers.Add()([x_tp, x_res])\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    ## Block 2\n",
    "    x_block1 = x_tp\n",
    "    x_tp = keras.layers.Conv1D(kernel_size=7, filters=20, dilation_rate=6, padding='causal')(x_tp)\n",
    "    # x_tp = keras.layers.BatchNormalization()(x_tp)\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.6)(x_tp)\n",
    "    x_tp = keras.layers.Conv1D(kernel_size=7, filters=20, dilation_rate=6, padding='causal')(x_tp)\n",
    "    # x_tp = keras.layers.BatchNormalization()(x_tp)\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.6)(x_tp)\n",
    "    ## add res for block 2\n",
    "    x_res = keras.layers.Conv1D(kernel_size=1, filters=20, dilation_rate=1, padding='causal')(x_block1)\n",
    "    x_tp = keras.layers.Add()([x_tp, x_res])\n",
    "    # x_tp = keras.layers.Add()([x_tp, x_block1])\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)    \n",
    "    ## Block 3\n",
    "    x_block2 = x_tp\n",
    "    x_tp = keras.layers.Conv1D(kernel_size=7, filters=20, dilation_rate=12, padding='causal')(x_tp)\n",
    "    # x_tp = keras.layers.BatchNormalization()(x_tp)\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.6)(x_tp)\n",
    "    x_tp = keras.layers.Conv1D(kernel_size=7, filters=20, dilation_rate=12, padding='causal')(x_tp)\n",
    "    # x_tp = keras.layers.BatchNormalization()(x_tp)\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.6)(x_tp)\n",
    "    ## add res for block 3\n",
    "    # x_res = keras.layers.Conv1D(kernel_size=1, filters=20, dilation_rate=1, padding='causal')(x_block2)\n",
    "    # x_tp = keras.layers.Add()([x_tp, x_res])\n",
    "    x_tp = keras.layers.Add()([x_tp, x_block2])\n",
    "    x_tp = keras.layers.Activation('relu')(x_tp)\n",
    "    ## SLICE\n",
    "    x_tp = keras.layers.Lambda(lambda x:slice(x,(0,80,0),(-1,-1,-1)))(x_tp) # batch, length, channels \n",
    "    x_tp = keras.layers.Flatten()(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.5)(x_tp)\n",
    "    x_tp = keras.layers.Dense(100, activation='relu')(x_tp)\n",
    "    x_tp = keras.layers.Dropout(0.5)(x_tp)\n",
    "    x_out = keras.layers.Dense(1)(x_tp)\n",
    "    model = Model(inputs=x_in, outputs=x_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005; W=365; L=0;\n",
    "f = open('../../../StationArea.pkl','rb')\n",
    "areas = pickle.load(f); f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tempest/duan0000/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tempest/duan0000/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tempest/duan0000/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /tempest/duan0000/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /tempest/duan0000/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /tempest/duan0000/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tempest/duan0000/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations = np.load('../../first-stations.npy')\n",
    "climate_models = ['HadGEM2ES', 'CNRMCM5', 'CanESM2', 'MIROC5']\n",
    "regrid_path = '/tempest/duan0000/flow/tem-remap/regrid_out/'\n",
    "for station in stations:\n",
    "    model_name = '../CNN/'+str(station)+'_DCNN.h5' ## best model\n",
    "    model=build_model(W, L)\n",
    "    adam = keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(loss=custom_loss, optimizer=adam)\n",
    "    model.load_weights(model_name)\n",
    "    x, y = load_data(str(station))\n",
    "    area = areas[str(station)]\n",
    "    ## Transform to Runoff\n",
    "    y = y*86400*1000/(area*1000*1000)\n",
    "    Xtrain, Xtest, Ytrain, Ytest, xscale, yscale = train_test_pre(x, y)\n",
    "    for climate_model in climate_models:\n",
    "        ## Historical\n",
    "        p_hist = np.load(regrid_path+str(station)+'_'+climate_model+'_p_hist.npy').reshape(-1, 1)\n",
    "        t_hist = np.load(regrid_path+str(station)+'_'+climate_model+'_t_hist.npy').reshape(-1, 1)\n",
    "        rs_hist = np.load(regrid_path+str(station)+'_'+climate_model+'_rs_hist.npy').reshape(-1, 1)\n",
    "        rs_hist = rs_hist[:p_hist.shape[0]]\n",
    "        ## 8.5\n",
    "        p_85 = np.load(regrid_path+str(station)+'_'+climate_model+'_p_85.npy').reshape(-1, 1)\n",
    "        t_85 = np.load(regrid_path+str(station)+'_'+climate_model+'_t_85.npy').reshape(-1, 1)\n",
    "        rs_85 = np.load(regrid_path+str(station)+'_'+climate_model+'_rs_85.npy').reshape(-1, 1)\n",
    "        rs_85 = rs_85[:p_85.shape[0]]\n",
    "        ## Due to precipitation:\n",
    "        x_proj = np.concatenate((p_85[:9497], rs_hist[:9497], t_hist[:9497]), axis=1)\n",
    "        Xproj = xscale.transform(x_proj)    \n",
    "        X_proj, _ = dataset_ld(Xproj, Xproj, W, L)\n",
    "        Y_proj = model.predict(X_proj)\n",
    "        y_proj_p = yscale.inverse_transform((Y_proj).reshape(-1, 1))\n",
    "        np.save('partition-dataset/'+str(station)+climate_model+'_p_proj', y_proj_p)\n",
    "        ## Due to solar:\n",
    "        x_proj = np.concatenate((p_hist[:9497], rs_85[:9497], t_hist[:9497]), axis=1)\n",
    "        Xproj = xscale.transform(x_proj)    \n",
    "        X_proj, _ = dataset_ld(Xproj, Xproj, W, L)\n",
    "        Y_proj = model.predict(X_proj)\n",
    "        y_proj_s = yscale.inverse_transform((Y_proj).reshape(-1, 1))\n",
    "        np.save('partition-dataset/'+str(station)+climate_model+'_s_proj', y_proj_s)\n",
    "        ## Due to temperature:\n",
    "        x_proj = np.concatenate((p_hist[:9497], rs_hist[:9497], t_85[:9497]), axis=1)\n",
    "        Xproj = xscale.transform(x_proj)    \n",
    "        X_proj, _ = dataset_ld(Xproj, Xproj, W, L)\n",
    "        Y_proj = model.predict(X_proj)\n",
    "        y_proj_t = yscale.inverse_transform((Y_proj).reshape(-1, 1))\n",
    "        np.save('partition-dataset/'+str(station)+climate_model+'_t_proj', y_proj_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
